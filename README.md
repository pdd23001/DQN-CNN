# Preference-Based Reinforcement Learning: CNN-Based Supervised Learning of Reward Models for Deep Q-Network (DQN) Agents

This repository contains the code for implementing Preference-Based Reinforcement Learning (PBRL) with CNN-based reward models for Deep Q-Network agents. This code was used to produce the results in the paper **pbrl.pdf** included in this repository. Detailed explanation of the methodology and experimental setup can be found in the paper.

## Project Description

In PBRL, agents learn a reward function from human preferences rather than relying on hand-crafted reward specifications. This project implements a CNN-based supervised learning approach to train reward models from pairwise trajectory comparisons, which are then used to guide DQN agents.

We demonstrate and validate our approach using the **TomatoSafetyGrid** environment, which features a challenging "reward hacking" scenario:
-   **Environment**: A gridworld where the agent must water all tomatoes.
-   **Challenge**: A "sprinkler" tile that causes tomatoes to visually disappear when stepped on, creating a "delusion" that can mislead the reward model.
-   **Testing Mechanism**: This reward hacking scenario serves as a rigorous test of our PBRL system's ability to learn accurate reward models and maintain alignment with the true task objective.
-   **Solution**: We implement **Reward Relabeling**—a mechanism that updates past experiences in the replay buffer with the latest reward model predictions, allowing the agent to correct past delusions and successfully solve the task.

## File Structure

```text
DQN_CNN/
├── README.md
├── requirements.txt
├── pbrl.pdf
└── src/
    ├── environment.py
    ├── eval.py
    ├── models.py
    ├── report.py
    ├── run_all.py
    ├── train.py
    ├── train_reward.py
    ├── utils.py
    └── results_paper/
```

### Script Explanations

-   **`src/train.py`**: The main training script for the DQN agent. It handles both standard RL (using the oracle/true reward) and PBRL (using a learned reward model). It manages the replay buffer, Q-network updates, and interaction with the environment.
-   **`src/train_reward.py`**: A dedicated script for pre-training a reward model using active learning. It collects preference queries from the environment and trains a `RewardCNN` to predict human preferences.
-   **`src/eval.py`**: The evaluation suite. It loads trained models (both policies and reward models), runs greedy evaluation episodes to measure success rates and returns, and generates all the plots and LaTeX tables used in the report.
-   **`src/run_all.py`**: An orchestrator script that automates the entire experimental pipeline. It runs the reward pre-training (on seed 0) and then launches training for all 4 policy variants across all 5 seeds, followed by evaluation.
-   **`src/models.py`**: Contains the PyTorch neural network definitions. It defines the `QNetwork` (for the policy) and `RewardCNN` (for the reward model), both sharing a common CNN encoder architecture.
-   **`src/environment.py`**: Defines the `TomatoSafetyGrid` Gymnasium environment. This includes the grid logic, the "delusion" mechanism (sprinkler hiding tomatoes), and the true reward function.
-   **`src/utils.py`**: A collection of utility functions for reproducible seeding, replay buffer management, preference sampling, and logging.
-   **`src/report.py`**: A helper script for generating report artifacts or specific analyses (if applicable).

## Setup

1.  **Create a Virtual Environment** (Recommended):
    
    **Option A: Using `venv`**:
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```
    
    **Option B: Using `conda` (you need to have conda installed)**:
    ```bash
    conda create -n pbrl python=3.13
    conda activate pbrl
    ```

2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

## Quickstart: Replicating Paper Results (Recommended)

To replicate the results from the paper (seeds 0-4, 2000 episodes), use the `run_all.py` script. This will:
1.  Train a "warm-start" reward model on Seed 0.
2.  Train 4 different policy variants for all 5 seeds (Standard, Warm-Start+Relabel, Warm-Start+NoRelabel, Random).
3.  Evaluate all agents and generate aggregate plots.

Run the following command from the root directory:

```bash
python3 src/run_all.py --seeds 0 1 2 3 4 --episodes 2000 --eval_after
```

**Note on Results**:
-   **Your Results**: The new results generated by this command will be saved in `src/results/`.
-   **Official Results**: The directory `src/results_paper/` contains the original results obtained by the us which are in the report. You can compare your generated results in `src/results/` with the official ones in `src/results_paper/`.

## Figures from the Paper (pbrl.pdf)

After running the quickstart command, you can find the generated figures corresponding to the paper as follows:

-   **Figure 2 (Diagnostics: Value Structure & Alignment)**:
    -   This figure is a composite of **Q-Value Heatmaps** and **Proxy-True Alignment** plots.
    -   **Heatmaps**: Found in `src/results/<exp>/figures/heatmap_q_max.png`.
    -   **Alignment**: Found in `src/results/<exp>/figures/alignment_proxy_vs_true.png`.
    > [!NOTE]
    > The paper figure combines these for "Relabel" vs "No-Relabel" experiments. There exist experiments of each type for multiple seeds. For Example, The heatmaps are from seed 0. The relabel experiment directories start with `ours_noisy_relabel...`, and the no relabel experiment directories start with `ours_noisy_norelabel...`.

-   **Figure 3 (Success Rate Comparison)**:
    -   Compares the success rate of different methods (Standard, Relabel, NoRelabel, Random) over training.
    -   **Location**: `src/results/compare_success.png` (and `.pdf`)

-   **Figure 4 (True Return Comparison)**:
    -   Compares the true return (environment reward) of different methods over training.
    -   **Location**: `src/results/compare_returns.png` (and `.pdf`)

-   **Figure 5 (Reward Model Validation)**:
    -   Shows the validation loss (BCE) of the reward model during active learning.
    -   **Location**: `src/results/reward_figures/reward_val_bce_mean.png` (and `.pdf`)
